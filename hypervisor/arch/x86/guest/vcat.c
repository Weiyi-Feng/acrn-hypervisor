/*
 * Copyright (C) 2021 Intel Corporation. All rights reserved.
 *
 * SPDX-License-Identifier: BSD-3-Clause
 */

#include <types.h>
#include <errno.h>
#include <logmsg.h>
#include <asm/cpufeatures.h>
#include <asm/cpuid.h>
#include <asm/rdt.h>
#include <asm/lib/bits.h>
#include <asm/board.h>
#include <asm/vm_config.h>
#include <asm/msr.h>
#include <asm/guest/vcpu.h>
#include <asm/guest/vm.h>
#include <asm/guest/vcat.h>
#include <asm/per_cpu.h>

/*
 * List of acronyms used here:
 *
 * - CAT:
 *  Cache Allocation Technology
 *
 *- vCAT:
 *  Virtual CAT
 *
 *- MSRs:
 *  Machine Specific Registers, each MSR is identified by a 32-bit integer.
 *
 *- pMSR:
 *  physical MSR
 *
 *- vMSR:
 *  virtual MSR
 *
 *- COS/CLOS:
 *  Class of Service. Also mean COS MSRs
 *
 *- CLOSID:
 *  Each CLOS has a number ID, ranges from 0 to COS_MAX
 *
 *- CLOSIDn:
 *  Each CLOS has a number ID denoted by n
 *
 *- COS_MAX:
 *  Max number of COS MSRs. ACRN uses the smallest number of
 *  CLOSIDs of all supported resources as COS_MAX to have consistent
 *  allocation
 *
-*  pCLOSID:
 *  Physical CLOSID
 *
 *- vCLOSID:
 *  Virtual CLOSID
 *
 *- MSR_IA32_type_MASK_n
 *  type: L2 or L3
 *  One CAT (CBM) MSR, where n corresponds to a number (CLOSIDn)
 *
 *- CBM:
 *  Capacity bitmask (cache bit mask), specifies which region of cache
 *  can be filled into, all (and only) contiguous '1' combinations are allowed
 *
 *- pCBM:
 *  Physical CBM
 *
 *- pCBM length (pcbm_len):
 *  pcbm_len is calculated by `bitmap_weight(max_pcbm)`
 *  indicates number of bits set in max_pcbm
 *
 *- max_pcbm (maximum physical cache space assigned to VM):
 *  max_pcbm is a contiguous capacity bitmask (CBM) starting at bit position low
 *  (the lowest assigned physical cache way) and ending at position high
 *  (the highest assigned physical cache way, inclusive).
 *  As CBM only allows contiguous '1' combinations, so max_pcbm essentially
 *  is a bitmask that selects/covers all the physical cache ways assigned to the VM.
 *
 *  Example:
 *  pcbm_len=20
 *  max_pcbm=0xfffff
 *
 *- CLOS_MASK/max_pcbm: (maximum assigned/reserved physical cache space)
 *  vCAT is built on top of RDT, vCAT on ACRN is enabled by configuring the FEATURES/RDT
 *  and vm sub-sections of the scenario XML file as in the below example:
 *  <RDT>
 *    <RDT_ENABLED>y</RDT_ENABLED>
 *    <CDP_ENABLED>n</CDP_ENABLED>
 *    <VCAT_ENABLED>y</VCAT_ENABLED>
 *    <CLOS_MASK>0x7ff</CLOS_MASK>
 *    <CLOS_MASK>0x7ff</CLOS_MASK>
 *    <CLOS_MASK>0x7ff</CLOS_MASK>
 *    <CLOS_MASK>0xff800</CLOS_MASK>
 *    <CLOS_MASK>0xff800</CLOS_MASK>
 *    <CLOS_MASK>0xff800</CLOS_MASK>
 *    <CLOS_MASK>0xff800</CLOS_MASK>
 *    <CLOS_MASK>0xff800</CLOS_MASK>
 *  /RDT>
 *
 *  <vm id="0">
 *   <guest_flags>
       <guest_flag>GUEST_FLAG_VCAT_ENABLED</guest_flag>
     </guest_flags>
 *   <clos>
 *     <vcpu_clos>3</vcpu_clos>
 *     <vcpu_clos>4</vcpu_clos>
 *     <vcpu_clos>5</vcpu_clos>
 *     <vcpu_clos>6</vcpu_clos>
 *     <vcpu_clos>7</vcpu_clos>
 *   </clos>
 *  </vm>
 *
 *  <vm id="1">
 *   <clos>
 *     <vcpu_clos>1</vcpu_clos>
 *     <vcpu_clos>2</vcpu_clos>
 *   </clos>
 *  </vm>
 *
 * vm_configurations.c (generated by config-tools) with the above vCAT config:
 *
 *  static uint16_t vm0_vcpu_clos[5U] = {3U, 4U, 5U, 6U, 7U};
 *  static uint16_t vm1_vcpu_clos[2U] = {1U, 2U};
 *
 *  struct acrn_vm_config vm_configs[CONFIG_MAX_VM_NUM] = {
 *  {
 *  .guest_flags = (GUEST_FLAG_VCAT_ENABLED),
 *  .pclosids = vm0_vcpu_clos,
 *  .num_pclosids = 5U,
 *  .max_l3_pcbm = 0xff800U,
 *  },
 *  {
 *  .pclosids = vm1_vcpu_clos,
 *  .num_pclosids = 2U,
 *  },
 *  };
 *
 *  Config CLOS_MASK/max_pcbm per pCLOSID:
 *    vCAT is enabled by setting both RDT_ENABLED and VCAT_ENABLED to 'y',
 *    then specify the GUEST_FLAG_VCAT_ENABLED guest flag for the desired VMs.
 *    Each CLOS_MASK (a.k.a. max_pcbm) setting corresponds to a pCLOSID and
 *    specifies the allocated portion (ways) of cache.
 *    For example, if COS_MAX is 7, then 8 CLOS_MASK settings need to be in place
 *    where each setting corresponds to a pCLOSID starting from 0.
 *    Each CLOS_MASK may or may not overlap with the CLOS_MASK of another pCLOSID depending
 *    on whether overlapped or isolated bitmask is desired for particular performance
 *    consideration.
 *
 *  Assign pCLOSIDs per VM
 *    Assign the desired pCLOSIDs to each VM in the vm/clos section of the scenario file
 *    by defining the vcpu_clos settings.
 *    All pCLOSIDs should be configured with the same pCBM (max_pcbm) to simplify vCAT
 *    config and ensure vCAT capability symmetry across cpus of the VM. In the above example,
 *    pCLOSIDs 3 to 7 are all configured with the same pCBM value 0xff800, which
 *    means a total of 9 physical cache ways have been reserved for all the cpus
 *    belonging to VM0
 *
 *- vCBM:
 *  Virtual CBM
 *
 *- vCBM length (vcbm_len):
 *  max number of bits to set for vCBM.
 *  vcbm_len is set equal to pcbm_len
 *  vCBM length is reported to guest VMs by using vCPUID (EAX=10H)
 *
 *- max_vcbm (maximum virtual cache space):
 *  Fully open vCBM (all ones bitmask), max vCBM is calculated
 *  by `(1 << vcbm_len) - 1`
 *
 *  Usually, vCLOSID0 is associated with the fully open vCBM to access all assigned virtual caches
 */

static int32_t wrmsr_pqr(struct acrn_vcpu *vcpu, uint64_t val);
static int32_t wrmsr_cbm(struct acrn_vcpu *vcpu, uint32_t msr, uint64_t val, int res);

/**
 * @brief Map vCAT MSR address to zero based index
 *
 * @pre  ((msr >= MSR_IA32_L3_MASK_BASE) && msr < (MSR_IA32_L3_MASK_BASE + NUM_VCAT_L3_MSRS))
 *       || ((msr >= MSR_IA32_L2_MASK_BASE) && msr < (MSR_IA32_L2_MASK_BASE + NUM_VCAT_L2_MSRS))
 *       || (msr == MSR_IA32_PQR_ASSOC)
 */
uint32_t vcat_msr2index(uint32_t msr)
{
	uint32_t index = 0U;

	/*  L3 MSRs indices assignment for MSR_IA32_L3_MASK_BASE ~ (MSR_IA32_L3_MASK_BASE + NUM_VCAT_L3_MSRS):
	 *  0
	 *  1
	 *  ...
	 *  (NUM_VCAT_L3_MSRS - 1)
	 *
	 *  L2 MSRs indices assignment:
	 *  NUM_VCAT_L3_MSRS
	 *  ...
	 *  NUM_VCAT_L3_MSRS + NUM_VCAT_L2_MSRS - 1

	 *  PQR index assignment for MSR_IA32_PQR_ASSOC:
	 *  NUM_VCAT_L3_MSRS
	 */

	if ((msr >= MSR_IA32_L3_MASK_BASE) && (msr < (MSR_IA32_L3_MASK_BASE + NUM_VCAT_L3_MSRS))) {
		index = msr - MSR_IA32_L3_MASK_BASE;
	} else if ((msr >= MSR_IA32_L2_MASK_BASE) && (msr < (MSR_IA32_L2_MASK_BASE + NUM_VCAT_L2_MSRS))) {
		index = msr - MSR_IA32_L2_MASK_BASE + NUM_VCAT_L3_MSRS;
	} else if (msr == MSR_IA32_PQR_ASSOC) {
		index = NUM_VCAT_L3_MSRS + NUM_VCAT_L2_MSRS;
	} else {
		ASSERT(false, "invalid msr address");
	}

	return index;
}

/**
 * @pre vm != NULL
 */
bool is_vcat_enabled(const struct acrn_vm *vm)
{
	return is_vcat_configured(vm) && ((get_rdt_res_cap_info(RDT_RESOURCE_L2)->num_closids > 0U)
			|| (get_rdt_res_cap_info(RDT_RESOURCE_L3)->num_closids > 0U));
}

/**
 * @pre vm != NULL
 */
bool is_l2_vcat_enabled(const struct acrn_vm *vm)
{
	return is_vcat_configured(vm) && (get_rdt_res_cap_info(RDT_RESOURCE_L2)->num_closids > 0U);
}

/**
 * @pre vm != NULL
 */
bool is_l3_vcat_enabled(const struct acrn_vm *vm)
{
	return is_vcat_configured(vm) && (get_rdt_res_cap_info(RDT_RESOURCE_L3)->num_closids > 0U);
}

/**
 * @brief Return number of vCLOSIDs of this VM
 *
 * @pre vm != NULL && vm->vm_id < CONFIG_MAX_VM_NUM
 */
uint16_t vcat_get_num_vclosids(const struct acrn_vm *vm)
{
	uint16_t num_vclosids = 0U;

	if (is_vcat_enabled(vm)) {
		/*
		 * For performance and simplicity, here number of vCLOSIDs (num_vclosids) is set
		 * equal to the number of pCLOSIDs assigned to this VM (get_vm_config(vm->vm_id)->num_pclosids).
		 * But technically, we do not have to make such an assumption. For example,
		 * Hypervisor could implement CLOSID context switch, then number of vCLOSIDs
		 * can be greater than the number of pCLOSIDs assigned. etc.
		 */
		num_vclosids = get_vm_config(vm->vm_id)->num_pclosids;
	}

	return num_vclosids;
}

/**
 * @brief Map vCLOSID to pCLOSID
 *
 * @pre vm != NULL && vm->vm_id < CONFIG_MAX_VM_NUM
 * @pre (get_vm_config(vm->vm_id)->pclosids != NULL) && (vclosid < get_vm_config(vm->vm_id)->num_pclosids)
 */
static uint32_t vclosid_to_pclosid(const struct acrn_vm *vm, uint32_t vclosid)
{
	ASSERT(vclosid < vcat_get_num_vclosids(vm), "vclosid is out of range!");

	/*
	 * pclosids points to an array of assigned pCLOSIDs
	 * Use vCLOSID as the index into the pclosids array, returning the corresponding pCLOSID
	 *
	 * Note that vcat_wrmsr() calls vclosid_to_pclosid() indirectly, in vcat_wrmsr(),
	 * the is_l2_cbm_vmsr()/is_l3_cbm_vmsr() calls ensure that vclosid is always less than
	 * get_vm_config(vm->vm_id)->num_pclosids, so vclosid is always an array index within bound here
	 */
	return get_vm_config(vm->vm_id)->pclosids[vclosid];
}

/**
 * @brief Return the max_pcbm of this VM.
 * @pre vm != NULL && vm->vm_id < CONFIG_MAX_VM_NUM
 * @pre res == RDT_RESOURCE_L2 || res == RDT_RESOURCE_L3
 */
static uint32_t get_max_pcbm(const struct acrn_vm *vm, int res)
{
	uint32_t max_pcbm = 0U;

	if (is_l2_vcat_enabled(vm) && (res == RDT_RESOURCE_L2)) {
		max_pcbm = get_vm_config(vm->vm_id)->max_l2_pcbm;
	} else if (is_l3_vcat_enabled(vm) && (res == RDT_RESOURCE_L3)) {
		max_pcbm = get_vm_config(vm->vm_id)->max_l3_pcbm;
	}

	return max_pcbm;
}

/**
 * @brief Retrieve vcbm_len of vm
 * @pre vm != NULL
 */
uint16_t vcat_get_vcbm_len(const struct acrn_vm *vm, int res)
{
	/* vcbm_len = pcbm_len */
	return bitmap_weight((uint64_t)get_max_pcbm(vm, res));
}

/**
 * @brief Retrieve max_vcbm of vm
 * @pre vm != NULL
 */
uint32_t vcat_get_max_vcbm(const struct acrn_vm *vm, int res)
{
	uint16_t vcbm_len = vcat_get_vcbm_len(vm, res);
	uint32_t max_vcbm = 0U;

	if (vcbm_len != 0U) {
		max_vcbm = (1U << vcbm_len) - 1U;
	}

	return max_vcbm;
}

/**
 * @brief Init vCBM and pCBM for MSR_IA32_type_MASK_n
 * (where type: L2 or L3) MSRs. The initial vCBM covers all the virtual cache ways the
 * guest VM may access, i.e. the superset bitmask.
 *
 * @pre vcpu != NULL && vcpu->vm != NULL
 */
static void init_cbm_msrs(struct acrn_vcpu *vcpu, int res, uint32_t msr_base)
{
	uint32_t max_vcbm = vcat_get_max_vcbm(vcpu->vm, res);

	if (max_vcbm != 0U) {
		uint32_t msr;
		uint16_t num_vclosids = vcat_get_num_vclosids(vcpu->vm);

		/* Initial vCBM is set to max_vcbm (maximum virtual cache space) */
		for (msr = msr_base; msr < (msr_base + num_vclosids); msr++) {
			/* Call wrmsr_cbm() to set vCBM (will also set pCBM internally) */
			(void)wrmsr_cbm(vcpu, msr, max_vcbm, res);
		}
	}
}

/**
 * @brief Init vCLOSID/pCLOSID for MSR_IA32_PQR_ASSOC MSR
 *
 * @pre vcpu != NULL
 */
static void init_pqr_msr(struct acrn_vcpu *vcpu)
{
	/* Set initial vCLOSID to 0 */
	uint64_t val = clos2pqr_msr(0U);

	/*
	 * Call wrmsr_pqr() to perform two actions:
	 * Set vCLOSID to 0
	 * Write the pCLOSID (corresponding to vCLOSID 0) to the guest msr area
	 */
	(void)wrmsr_pqr(vcpu, val);
}

/**
 * @pre vcpu != NULL
 */
static void init_vcat_msrs_internal(struct acrn_vcpu *vcpu)
{
	/* Init MSR_IA32_L2_MASK_n MSRs */
	init_cbm_msrs(vcpu, RDT_RESOURCE_L2, MSR_IA32_L2_MASK_BASE);

	/* Init MSR_IA32_L3_MASK_n MSRs */
	init_cbm_msrs(vcpu, RDT_RESOURCE_L3, MSR_IA32_L3_MASK_BASE);

	/* Init MSR_IA32_PQR_ASSOC MSR */
	init_pqr_msr(vcpu);
}

/**
 * @pre vcpu != NULL && vcpu->vm != NULL
 */
void init_vcat_msrs(struct acrn_vcpu *vcpu)
{
	if (is_vcat_enabled(vcpu->vm)) {
		init_vcat_msrs_internal(vcpu);
	}
}

/**
 * @brief Map pCBM to vCBM
 *
 * @pre vm != NULL
 */
uint32_t vcat_pcbm_to_vcbm(const struct acrn_vm *vm, uint32_t pcbm, int res)
{
	/*
	 * max_pcbm/CLOS_MASK is defined in scenario file and is a contiguous bitmask starting
	 * at bit position low (the lowest assigned physical cache way) and ending at position
	 * high (the highest assigned physical cache way, inclusive). As CBM only allows
	 * contiguous '1' combinations, so max_pcbm essentially is a bitmask that selects/covers
	 * all the physical cache ways assigned to the VM.
	 *
	 * For illustrative purpose, here we assume that we have the two functions
	 * GENMASK() and BIT() defined as follows:
	 * GENMASK(high, low): create a contiguous bitmask starting at bit position low and
	 * ending at position high, inclusive.
	 * BIT(n): create a bitmask with bit n set.
	 *
	 * max_pcbm, min_pcbm, max_vcbm, min_vcbm and the relationship between them
	 * can be expressed as:
	 * max_pcbm = GENMASK(high, low)
	 * min_pcbm = BIT(low)
	 *
	 * max_vcbm = GENMASK(high - low, 0)
	 * min_vcbm = BIT(0)
	 *
	 * pcbm to vcbm conversion (mask off the unwanted bits to prevent erroneous mask values):
	 * vcbm = (pcbm & max_pcbm) >> low
	 *
	 * max_pcbm will be mapped to max_vcbm
	 * min_pcbm will be mapped to min_vcbm
	 */
	uint32_t max_pcbm = get_max_pcbm(vm, res);

	/* Find the position low (the first bit set) in max_pcbm */
	uint16_t low = ffs64((uint64_t)max_pcbm);

	/* pcbm set bits should only be in the range of [low, high] */
	return (pcbm & max_pcbm) >> low;
}

/**
 * @brief MSR_IA32_type_MASK_n MSRs write handler
 *
 * @pre vcpu != NULL && vcpu->vm != NULL
 */
static int32_t wrmsr_cbm(__unused struct acrn_vcpu *vcpu, __unused uint32_t msr, __unused uint64_t val, __unused int res)
{
	/*TODO: this is going to be implemented in a subsequent commit */
	return -EFAULT;
}

/**
 * @brief MSR_IA32_PQR_ASSOC MSR write handler
 *
 * @pre vcpu != NULL && vcpu->vm != NULL
 */
static int32_t wrmsr_pqr(struct acrn_vcpu *vcpu, uint64_t val)
{
	uint32_t vclosid = (uint32_t)((val >> 32U) & 0xFFFFFFFFUL);
	uint32_t pclosid = vclosid_to_pclosid(vcpu->vm, vclosid);
	uint64_t pmsr_value = clos2pqr_msr(pclosid);

	/* Write the new pCLOSID value to the guest msr area */
	vcpu->arch.msr_area.guest[MSR_AREA_IA32_PQR_ASSOC].value = pmsr_value;

	vcpu_set_guest_msr(vcpu, MSR_IA32_PQR_ASSOC, val);

	return 0;
}
